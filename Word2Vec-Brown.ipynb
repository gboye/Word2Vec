{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893d4b71",
   "metadata": {},
   "source": [
    "# Calcul des vecteurs Word2Vec pour Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e69d0",
   "metadata": {},
   "source": [
    "## Préparatifs\n",
    "- importer pandas pour les tableaux\n",
    "- importer spatial pour les cosinus\n",
    "- récupérer le tableau des hétérographes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb54eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from scipy import spatial\n",
    "from scipy.spatial.distance import squareform, pdist, cosine\n",
    "import itertools as it\n",
    "\n",
    "#lecture du tableau sauvé par Triplets Brown\n",
    "%store -r dfHeterographes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4f9d2",
   "metadata": {},
   "source": [
    "## Import de Brown via nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67b3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/gilles/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gilles/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabdc1d",
   "metadata": {},
   "source": [
    "## Import de gensim pour Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511beaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "#tuto: https://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8d450",
   "metadata": {},
   "source": [
    "## Tokenisation de Brown\n",
    "filtrage des phrases pour éliminer :\n",
    "- les nombres\n",
    "- la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24227678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenisation\n",
    "corpus = brown.sents()\n",
    "list_stopwords= set(stopwords.words('english'))\n",
    "list_sentences_tok = []\n",
    "for e in range(len(corpus)):\n",
    "    list_to_tok = corpus[e]\n",
    "    list_tok_words_in_sentence = []\n",
    "    for i in range(len(list_to_tok)):\n",
    "        word = list_to_tok[i]\n",
    "        word = word.lower()\n",
    "#         if word.isdigit():\n",
    "#             print(word)\n",
    "#         else:\n",
    "        if (not word.isdigit()) and (word != \".\") and (word != \",\") and (word != \"!\") and (word != \"?\") and (word != \":\") and (\n",
    "                    word != \"``\") and (word != \"\\'\\'\") and (word != \";\") and (word != \"-\") and (word != \"--\") and (\n",
    "                    word != \"(\") and (word != \")\") and (word != \"\\n\") and (word != \"\\b\"):\n",
    "                list_tok_words_in_sentence.append(word)\n",
    "\n",
    "    list_sentences_tok.append(list_tok_words_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9497c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place'], ['the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted'], ['the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', 'irregularities', 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr.'], ['only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', 'the', 'jury', 'said', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city'], ['the', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"georgia's\", 'registration', 'and', 'election', 'laws', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list_sentences_tok[:5])\n",
    "len(list_sentences_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb9272",
   "metadata": {},
   "source": [
    "## Entraîner le modèle\n",
    "- min_count : la fréquence minimale pour avoir un vecteur\n",
    "- size : nombre de neurones en entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9af72d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "minF=15\n",
    "nNeurones=150\n",
    "nIter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c7ebbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 11:48:41,460 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2021-11-20 11:48:41,464 : INFO : collecting all words and their counts\n",
      "2021-11-20 11:48:41,466 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-20 11:48:41,529 : INFO : PROGRESS: at sentence #10000, processed 191634 words, keeping 20884 word types\n",
      "2021-11-20 11:48:41,593 : INFO : PROGRESS: at sentence #20000, processed 376039 words, keeping 30258 word types\n",
      "2021-11-20 11:48:41,665 : INFO : PROGRESS: at sentence #30000, processed 585173 words, keeping 37149 word types\n",
      "2021-11-20 11:48:41,727 : INFO : PROGRESS: at sentence #40000, processed 777469 words, keeping 43020 word types\n",
      "2021-11-20 11:48:41,774 : INFO : PROGRESS: at sentence #50000, processed 905587 words, keeping 46421 word types\n",
      "2021-11-20 11:48:41,810 : INFO : collected 49119 word types from a corpus of 1007462 raw words and 57340 sentences\n",
      "2021-11-20 11:48:41,811 : INFO : Loading a fresh vocabulary\n",
      "2021-11-20 11:48:42,293 : INFO : effective_min_count=15 retains 6225 unique words (12% of original 49119, drops 42894)\n",
      "2021-11-20 11:48:42,294 : INFO : effective_min_count=15 leaves 887927 word corpus (88% of original 1007462, drops 119535)\n",
      "2021-11-20 11:48:42,320 : INFO : deleting the raw counts dictionary of 49119 items\n",
      "2021-11-20 11:48:42,322 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2021-11-20 11:48:42,323 : INFO : downsampling leaves estimated 639018 word corpus (72.0% of prior 887927)\n",
      "2021-11-20 11:48:42,350 : INFO : estimated required memory for 6225 words and 150 dimensions: 10582500 bytes\n",
      "2021-11-20 11:48:42,351 : INFO : resetting layer weights\n",
      "2021-11-20 11:48:43,600 : INFO : training model with 3 workers on 6225 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-11-20 11:48:44,491 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:44,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:44,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:44,497 : INFO : EPOCH - 1 : training on 1007462 raw words (639149 effective words) took 0.9s, 716957 effective words/s\n",
      "2021-11-20 11:48:45,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:45,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:45,472 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:45,473 : INFO : EPOCH - 2 : training on 1007462 raw words (639221 effective words) took 1.0s, 663644 effective words/s\n",
      "2021-11-20 11:48:46,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:46,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:46,421 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:46,422 : INFO : EPOCH - 3 : training on 1007462 raw words (638627 effective words) took 0.9s, 680363 effective words/s\n",
      "2021-11-20 11:48:47,348 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:47,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:47,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:47,359 : INFO : EPOCH - 4 : training on 1007462 raw words (638661 effective words) took 0.9s, 690711 effective words/s\n",
      "2021-11-20 11:48:48,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:48,254 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:48,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:48,259 : INFO : EPOCH - 5 : training on 1007462 raw words (638810 effective words) took 0.9s, 718355 effective words/s\n",
      "2021-11-20 11:48:49,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:49,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:49,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:49,128 : INFO : EPOCH - 6 : training on 1007462 raw words (639131 effective words) took 0.9s, 744666 effective words/s\n",
      "2021-11-20 11:48:50,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:50,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:50,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:50,027 : INFO : EPOCH - 7 : training on 1007462 raw words (639339 effective words) took 0.9s, 719140 effective words/s\n",
      "2021-11-20 11:48:50,902 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:50,909 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:50,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:50,911 : INFO : EPOCH - 8 : training on 1007462 raw words (638570 effective words) took 0.9s, 730210 effective words/s\n",
      "2021-11-20 11:48:51,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:51,782 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:51,788 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:51,789 : INFO : EPOCH - 9 : training on 1007462 raw words (639495 effective words) took 0.9s, 734226 effective words/s\n",
      "2021-11-20 11:48:52,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-20 11:48:52,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-20 11:48:52,693 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-20 11:48:52,694 : INFO : EPOCH - 10 : training on 1007462 raw words (639274 effective words) took 0.9s, 713557 effective words/s\n",
      "2021-11-20 11:48:52,694 : INFO : training on a 10074620 raw words (6390277 effective words) took 9.1s, 702731 effective words/s\n",
      "2021-11-20 11:48:52,698 : INFO : saving Word2Vec object under w2v-min15-s150-it10, separately None\n",
      "2021-11-20 11:48:52,699 : INFO : not storing attribute vectors_norm\n",
      "2021-11-20 11:48:52,700 : INFO : not storing attribute cum_table\n",
      "2021-11-20 11:48:52,782 : INFO : saved w2v-min15-s150-it10\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(list_sentences_tok, min_count=minF, size=nNeurones, iter=nIter )\n",
    "model.save(\"w2v-min%d-s%d-it%d\"%(minF,nNeurones,nIter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17e60825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 11:47:46,238 : INFO : loading Word2Vec object from w2v-min10-s150-it10\n",
      "2021-11-20 11:47:46,352 : INFO : loading wv recursively from w2v-min10-s150-it10.wv.* with mmap=None\n",
      "2021-11-20 11:47:46,353 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-11-20 11:47:46,354 : INFO : loading vocabulary recursively from w2v-min10-s150-it10.vocabulary.* with mmap=None\n",
      "2021-11-20 11:47:46,355 : INFO : loading trainables recursively from w2v-min10-s150-it10.trainables.* with mmap=None\n",
      "2021-11-20 11:47:46,356 : INFO : setting ignored attribute cum_table to None\n",
      "2021-11-20 11:47:46,357 : INFO : loaded w2v-min10-s150-it10\n"
     ]
    }
   ],
   "source": [
    "#load and use Word2Vec for cosine similarities\n",
    "model = gensim.models.Word2Vec.load(\"w2v-min%d-s%d-it%d\"%(minF,nNeurones,nIter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82f6bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 11:49:11,916 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mary', 0.791098952293396),\n",
       " ('queen', 0.7890463471412659),\n",
       " ('richard', 0.7861088514328003),\n",
       " ('mrs.', 0.7743070721626282),\n",
       " ('judge', 0.7711563110351562),\n",
       " ('james', 0.7697207927703857),\n",
       " ('charles', 0.7640933990478516),\n",
       " ('anne', 0.7624502182006836),\n",
       " ('martin', 0.7595736980438232),\n",
       " ('john', 0.755841076374054)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335a950",
   "metadata": {},
   "source": [
    "## Calculer les vecteurs de translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7073bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorTrans(row):\n",
    "    w1=row[\"PART\"]\n",
    "    w2=row[\"PST\"]\n",
    "    result=model.wv[w1]-model.wv[w2]\n",
    "    return result\n",
    "\n",
    "sVectors=dfHeterographes[[\"PST\",\"PART\"]].apply(vectorTrans, axis=1)\n",
    "dfVectors = pd.DataFrame(sVectors.values.tolist(), index=sVectors.index)\n",
    "mVector=dfVectors.mean().to_list()\n",
    "fVector=dfVectors.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5d479",
   "metadata": {},
   "source": [
    "## Matrice de distance entre les vecteurs de translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8d048d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>know</th>\n",
       "      <th>go</th>\n",
       "      <th>take</th>\n",
       "      <th>come</th>\n",
       "      <th>give</th>\n",
       "      <th>become</th>\n",
       "      <th>show</th>\n",
       "      <th>write</th>\n",
       "      <th>begin</th>\n",
       "      <th>break</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553610</td>\n",
       "      <td>0.497953</td>\n",
       "      <td>0.753385</td>\n",
       "      <td>0.401039</td>\n",
       "      <td>0.591061</td>\n",
       "      <td>0.394530</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.509198</td>\n",
       "      <td>0.530561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0.553610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217976</td>\n",
       "      <td>0.326535</td>\n",
       "      <td>0.322603</td>\n",
       "      <td>0.340650</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.311981</td>\n",
       "      <td>0.140339</td>\n",
       "      <td>0.355973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>0.497953</td>\n",
       "      <td>0.217976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279765</td>\n",
       "      <td>0.151583</td>\n",
       "      <td>0.255757</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.216126</td>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.489984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.753385</td>\n",
       "      <td>0.326535</td>\n",
       "      <td>0.279765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404447</td>\n",
       "      <td>0.094676</td>\n",
       "      <td>0.600145</td>\n",
       "      <td>0.365932</td>\n",
       "      <td>0.343540</td>\n",
       "      <td>0.602688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give</th>\n",
       "      <td>0.401039</td>\n",
       "      <td>0.322603</td>\n",
       "      <td>0.151583</td>\n",
       "      <td>0.404447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381626</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>0.219991</td>\n",
       "      <td>0.306075</td>\n",
       "      <td>0.442274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>become</th>\n",
       "      <td>0.591061</td>\n",
       "      <td>0.340650</td>\n",
       "      <td>0.255757</td>\n",
       "      <td>0.094676</td>\n",
       "      <td>0.381626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485630</td>\n",
       "      <td>0.295559</td>\n",
       "      <td>0.330384</td>\n",
       "      <td>0.650169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>0.394530</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.600145</td>\n",
       "      <td>0.338384</td>\n",
       "      <td>0.485630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422517</td>\n",
       "      <td>0.503956</td>\n",
       "      <td>0.708772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.311981</td>\n",
       "      <td>0.216126</td>\n",
       "      <td>0.365932</td>\n",
       "      <td>0.219991</td>\n",
       "      <td>0.295559</td>\n",
       "      <td>0.422517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368348</td>\n",
       "      <td>0.498586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>begin</th>\n",
       "      <td>0.509198</td>\n",
       "      <td>0.140339</td>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.343540</td>\n",
       "      <td>0.306075</td>\n",
       "      <td>0.330384</td>\n",
       "      <td>0.503956</td>\n",
       "      <td>0.368348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>break</th>\n",
       "      <td>0.530561</td>\n",
       "      <td>0.355973</td>\n",
       "      <td>0.489984</td>\n",
       "      <td>0.602688</td>\n",
       "      <td>0.442274</td>\n",
       "      <td>0.650169</td>\n",
       "      <td>0.708772</td>\n",
       "      <td>0.498586</td>\n",
       "      <td>0.415901</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            know        go      take      come      give    become      show  \\\n",
       "know    0.000000  0.553610  0.497953  0.753385  0.401039  0.591061  0.394530   \n",
       "go      0.553610  0.000000  0.217976  0.326535  0.322603  0.340650  0.480198   \n",
       "take    0.497953  0.217976  0.000000  0.279765  0.151583  0.255757  0.313841   \n",
       "come    0.753385  0.326535  0.279765  0.000000  0.404447  0.094676  0.600145   \n",
       "give    0.401039  0.322603  0.151583  0.404447  0.000000  0.381626  0.338384   \n",
       "become  0.591061  0.340650  0.255757  0.094676  0.381626  0.000000  0.485630   \n",
       "show    0.394530  0.480198  0.313841  0.600145  0.338384  0.485630  0.000000   \n",
       "write   0.478464  0.311981  0.216126  0.365932  0.219991  0.295559  0.422517   \n",
       "begin   0.509198  0.140339  0.216549  0.343540  0.306075  0.330384  0.503956   \n",
       "break   0.530561  0.355973  0.489984  0.602688  0.442274  0.650169  0.708772   \n",
       "\n",
       "           write     begin     break  \n",
       "know    0.478464  0.509198  0.530561  \n",
       "go      0.311981  0.140339  0.355973  \n",
       "take    0.216126  0.216549  0.489984  \n",
       "come    0.365932  0.343540  0.602688  \n",
       "give    0.219991  0.306075  0.442274  \n",
       "become  0.295559  0.330384  0.650169  \n",
       "show    0.422517  0.503956  0.708772  \n",
       "write   0.000000  0.368348  0.498586  \n",
       "begin   0.368348  0.000000  0.415901  \n",
       "break   0.498586  0.415901  0.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(squareform(pdist(dfVectors,metric=\"cosine\")), columns=dfVectors.index.unique(), index=dfVectors.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec0cadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know      0.360613\n",
       "go        0.138104\n",
       "take      0.072979\n",
       "come      0.193054\n",
       "give      0.125248\n",
       "become    0.151097\n",
       "show      0.282243\n",
       "write     0.150110\n",
       "begin     0.151931\n",
       "break     0.385342\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dist2mean(row):\n",
    "    lVector=row.to_list()\n",
    "    return cosine(lVector,mVector)\n",
    "\n",
    "dfVectors.apply(dist2mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7551c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('went', 0.8225483894348145), ('stayed', 0.7553375959396362), ('came', 0.7502672672271729)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20803046226501465"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb=\"go\"\n",
    "pivot=\"begin\"\n",
    "f1=\"PART\"\n",
    "f2=\"PST\"\n",
    "nVoisins=3\n",
    "w1=dfHeterographes.loc[verb,f1]\n",
    "w2=dfHeterographes.loc[verb,f2]\n",
    "p1=dfHeterographes.loc[pivot,f1]\n",
    "p2=dfHeterographes.loc[pivot,f2]\n",
    "# model.wv.most_similar(positive=[\"gone\",\"knew\"],negative=[\"known\"])\n",
    "print(model.wv.most_similar(positive=[w1,p2],negative=[p1],topn=nVoisins))\n",
    "\n",
    "vCalc=model.wv[w1]-model.wv[p1]+model.wv[p2]\n",
    "vReal=model.wv[w2]\n",
    "cosine(vReal,vCalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "430cecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known knew known knew 999 0.28209859132766724\n",
      "know know 0\n",
      "known knew gone went 7 0.5632972717285156\n",
      "know go 0.4607241749763489\n",
      "known knew taken took 10 0.6094611883163452\n",
      "know take 0.420911967754364\n",
      "known knew come came 48 0.5171260237693787\n",
      "know come 0.5897088050842285\n",
      "known knew given gave 3 0.6997350454330444\n",
      "know give 0.32456058263778687\n",
      "known knew become became 9 0.6217175722122192\n",
      "know become 0.4475536346435547\n",
      "known knew shown showed 1 0.7598372101783752\n",
      "know show 0.2994845509529114\n",
      "known knew written wrote 19 0.6337090730667114\n",
      "know write 0.330044150352478\n",
      "known knew begun began 1 0.5989542007446289\n",
      "know begin 0.3395113945007324\n",
      "known knew broken broke 3 0.5968277454376221\n",
      "know break 0.3438059091567993\n",
      "gone went known knew 66 0.5470482707023621\n",
      "go know 0.4189510941505432\n",
      "gone went gone went 999 0.36178627610206604\n",
      "go go 0\n",
      "gone went taken took 1 0.7453521490097046\n",
      "go take 0.23381608724594116\n",
      "gone went come came 18 0.676718533039093\n",
      "go come 0.36773747205734253\n",
      "gone went given gave 7 0.6889999508857727\n",
      "go give 0.30865341424942017\n",
      "gone went become became 34 0.6374316215515137\n",
      "go become 0.37643176317214966\n",
      "gone went shown showed 64 0.5399700403213501\n",
      "go show 0.4924274682998657\n",
      "gone went written wrote 11 0.6971724033355713\n",
      "go write 0.3214340806007385\n",
      "gone went begun began 0 0.8225483894348145\n",
      "go begin 0.20803046226501465\n",
      "gone went broken broke 5 0.6430034041404724\n",
      "go break 0.40382665395736694\n",
      "taken took known knew 163 0.45727890729904175\n",
      "take know 0.49971359968185425\n",
      "taken took gone went 1 0.7063493728637695\n",
      "take go 0.2499237060546875\n",
      "taken took taken took 999 0.3286556601524353\n",
      "take take 0\n",
      "taken took come came 7 0.65185546875\n",
      "take come 0.33344972133636475\n",
      "taken took given gave 0 0.7808618545532227\n",
      "take give 0.2116258144378662\n",
      "taken took become became 1 0.6930739283561707\n",
      "take become 0.3161947727203369\n",
      "taken took shown showed 2 0.6373518705368042\n",
      "take show 0.4529556632041931\n",
      "taken took written wrote 0 0.7120815515518188\n",
      "take write 0.3099481463432312\n",
      "taken took begun began 2 0.6832236051559448\n",
      "take begin 0.3521760106086731\n",
      "taken took broken broke 64 0.4730234444141388\n",
      "take break 0.5754406750202179\n",
      "come came known knew 62 0.543737530708313\n",
      "come know 0.4116102457046509\n",
      "come came gone went 0 0.7661573886871338\n",
      "come go 0.21216821670532227\n",
      "come came taken took 1 0.736559271812439\n",
      "come take 0.23227202892303467\n",
      "come came come came 999 0.33942726254463196\n",
      "come come 0\n",
      "come came given gave 1 0.7185868620872498\n",
      "come give 0.2826020121574402\n",
      "come came become became 0 0.8886617422103882\n",
      "come become 0.10695964097976685\n",
      "come came shown showed 9 0.5838406682014465\n",
      "come show 0.46359580755233765\n",
      "come came written wrote 5 0.6811130046844482\n",
      "come write 0.310949444770813\n",
      "come came begun began 1 0.708511471748352\n",
      "come begin 0.27989262342453003\n",
      "come came broken broke 3 0.6267286539077759\n",
      "come break 0.4033430814743042\n",
      "given gave known knew 9 0.4955331087112427\n",
      "give know 0.43185603618621826\n",
      "given gave gone went 3 0.5796104073524475\n",
      "give go 0.38995516300201416\n",
      "given gave taken took 0 0.8049243688583374\n",
      "give take 0.17715901136398315\n",
      "given gave come came 10 0.5645830035209656\n",
      "give come 0.427642822265625\n",
      "given gave given gave 999 0.27836206555366516\n",
      "give give 0\n",
      "given gave become became 0 0.6270022988319397\n",
      "give become 0.4062400460243225\n",
      "given gave shown showed 0 0.6962944269180298\n",
      "give show 0.39179104566574097\n",
      "given gave written wrote 0 0.7551622986793518\n",
      "give write 0.2661042809486389\n",
      "given gave begun began 1 0.5230255126953125\n",
      "give begin 0.4507139325141907\n",
      "given gave broken broke 40 0.4352896511554718\n",
      "give break 0.6048615872859955\n",
      "become became known knew 270 0.35146576166152954\n",
      "become know 0.5349684357643127\n",
      "become became gone went 4 0.5334855318069458\n",
      "become go 0.45399606227874756\n",
      "become became taken took 0 0.6290907263755798\n",
      "become take 0.35948067903518677\n",
      "become became come came 0 0.8392201066017151\n",
      "become come 0.1455419659614563\n",
      "become became given gave 0 0.6106494665145874\n",
      "become give 0.40030592679977417\n",
      "become became become became 999 0.2827613651752472\n",
      "become become 0\n",
      "become became shown showed 7 0.5377675294876099\n",
      "become show 0.6193552911281586\n",
      "become became written wrote 0 0.6057299375534058\n",
      "become write 0.4019694924354553\n",
      "become became begun began 2 0.49303922057151794\n",
      "become begin 0.4965357184410095\n",
      "become became broken broke 48 0.38341736793518066\n",
      "become break 0.6835897564888\n",
      "shown showed known knew 12 0.5750618577003479\n",
      "show know 0.33357858657836914\n",
      "shown showed gone went 121 0.5091198682785034\n",
      "show go 0.48402345180511475\n",
      "shown showed taken took 1 0.6521999835968018\n",
      "show take 0.3335990309715271\n",
      "shown showed come came 40 0.5670575499534607\n",
      "show come 0.42833709716796875\n",
      "shown showed given gave 5 0.6331959962844849\n",
      "show give 0.3573431968688965\n",
      "shown showed become became 1 0.6536843776702881\n",
      "show become 0.3851720690727234\n",
      "shown showed shown showed 999 0.3454078435897827\n",
      "show show 0\n",
      "shown showed written wrote 30 0.5466382503509521\n",
      "show write 0.4249478578567505\n",
      "shown showed begun began 5 0.5235756039619446\n",
      "show begin 0.4664539694786072\n",
      "shown showed broken broke 215 0.37504467368125916\n",
      "show break 0.6023699641227722\n",
      "written wrote known knew 23 0.5600433945655823\n",
      "write know 0.3740878105163574\n",
      "written wrote gone went 2 0.6731866598129272\n",
      "write go 0.3990856409072876\n",
      "written wrote taken took 0 0.7663215398788452\n",
      "write take 0.27320539951324463\n",
      "written wrote come came 1 0.6869015693664551\n",
      "write come 0.3831378221511841\n",
      "written wrote given gave 0 0.7688077688217163\n",
      "write give 0.27122992277145386\n",
      "written wrote become became 0 0.7635017037391663\n",
      "write become 0.30626094341278076\n",
      "written wrote shown showed 3 0.6502127051353455\n",
      "write show 0.4269479513168335\n",
      "written wrote written wrote 999 0.357147216796875\n",
      "write write 0\n",
      "written wrote begun began 4 0.583112359046936\n",
      "write begin 0.33043527603149414\n",
      "written wrote broken broke 11 0.5646782517433167\n",
      "write break 0.4155420660972595\n",
      "begun began known knew 54 0.5984099507331848\n",
      "begin know 0.4373674988746643\n",
      "begun began gone went 0 0.8783934712409973\n",
      "begin go 0.1843123435974121\n",
      "begun began taken took 0 0.8219304084777832\n",
      "begin take 0.2703402638435364\n",
      "begun began come came 2 0.7450706362724304\n",
      "begin come 0.3609015941619873\n",
      "begun began given gave 0 0.7558887004852295\n",
      "begin give 0.3123413324356079\n",
      "begun began become became 1 0.7455655336380005\n",
      "begin become 0.36485064029693604\n",
      "begun began shown showed 17 0.6338632106781006\n",
      "begin show 0.5199686884880066\n",
      "begun began written wrote 1 0.7123193740844727\n",
      "begin write 0.31442558765411377\n",
      "begun began begun began 999 0.3167518377304077\n",
      "begin begin 0\n",
      "begun began broken broke 5 0.6540542840957642\n",
      "begin break 0.3087746500968933\n",
      "broken broke known knew 40 0.6847025752067566\n",
      "break know 0.43603843450546265\n",
      "broken broke gone went 27 0.7198136448860168\n",
      "break go 0.3878890872001648\n",
      "broken broke taken took 129 0.6229895353317261\n",
      "break take 0.519789308309555\n",
      "broken broke come came 163 0.642677366733551\n",
      "break come 0.5746751427650452\n",
      "broken broke given gave 59 0.6900429725646973\n",
      "break give 0.4598209857940674\n",
      "broken broke become became 148 0.5787523984909058\n",
      "break become 0.6197258532047272\n",
      "broken broke shown showed 190 0.5458112955093384\n",
      "break show 0.7099243998527527\n",
      "broken broke written wrote 85 0.658653974533081\n",
      "break write 0.45340847969055176\n",
      "broken broke begun began 113 0.6286590695381165\n",
      "break begin 0.3383067846298218\n",
      "broken broke broken broke 999 0.48451316356658936\n",
      "break break 0\n"
     ]
    }
   ],
   "source": [
    "def distCalcReal(verb,pivot):\n",
    "    w1=dfHeterographes.loc[verb,f1]\n",
    "    w2=dfHeterographes.loc[verb,f2]\n",
    "    p1=dfHeterographes.loc[pivot,f1]\n",
    "    p2=dfHeterographes.loc[pivot,f2]\n",
    "\n",
    "    vCalc=model.wv[w1]-model.wv[p1]+model.wv[p2]\n",
    "    vReal=model.wv[w2]\n",
    "#     print(model.wv.rank(vCalc, vReal))\n",
    "    neighbours=model.wv.most_similar(positive=[w1,p2],negative=[p1],topn=1000)\n",
    "    for nN, (n,c) in enumerate(neighbours):\n",
    "        if n==w2:\n",
    "            #print(nN,n,c)\n",
    "            break\n",
    "    print(w1, w2, p1, p2, nN, c)\n",
    "\n",
    "    return cosine(vReal,vCalc)\n",
    "\n",
    "for v in dfHeterographes.index:\n",
    "    for p in dfHeterographes.index:\n",
    "        print (v, p, distCalcReal(v,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc266372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6225"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.vocab.keys()))\n",
    "# 1-cosine(model.wv[w1],model.wv[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fdcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
